Abstract
Semantic matching, which aims to determine the matching degree between two texts, is a fundamen- tal problem for many NLP applications. Recently, deep learning approach has been applied to this problem and significant improvements have been achieved. In this paper, we propose to view the generation of the global interaction between two texts as a recursive process: i.e. the interaction of two texts at each position is a composition of the interactions between their prefixes as well as the word level interaction at the current position. Based on this idea, we propose a novel deep architec- ture, namely Match-SRNN, to model the recursive matching structure. Firstly, a tensor is constructed to capture the word level interactions. Then a spa- tial RNN is applied to integrate the local interac- tions recursively, with importance determined by four types of gates. Finally, the matching score is calculated based on the global interaction. We show that, after degenerated to the exact match- ing scenario, Match-SRNN can approximate the dynamic programming process of longest common subsequence. Thus, there exists a clear interpre- tation for Match-SRNN. Our experiments on two semantic matching tasks showed the effectiveness of Match-SRNN, and its ability of visualizing the learned matching structure.
1 Introduction
Semantic matching is a critical task for many applications in natural language processing, including information retrieval, question answering and paraphrase identification [Li and Xu, 2013]. The target of semantic matching is to determine a matching score for two given texts. Taking the task of ques- tion answering as an example, given a pair of question and an- swer, a matching function is created to determine the match- ing degree between these two texts. Traditional methods such as BM25 and feature based learning models usually rely on exact matching patterns to determine the degree, and thus suf- fer from the vocabulary mismatching problem [Li and Xu, 2013].
Recently, deep learning approach has been applied to this area and well tackled the vocabulary mismatching problem. Some existing work focus on representing each text as one or several dense vectors, and then calculate the matching score based on the similarity between these vectors. Exam- ples include RAE [Socher et al., 2011], DSSM [Huang et al., 2013], CDSSM [Shen et al., 2014], ARC-I [Hu et al., 2014], CNTN [Qiu and Huang, 2015], LSTM-RNN [Palangi et al., 2015], MultiGranCNN [Yin and Schu ̈tze, 2015a; Yin and Schu ̈tze, 2015b] and MV-LSTM [Wan et al., 2016]. However, it is usually difficult for these methods to model the complicated interaction relationship between two texts [Lu and Li, 2013] because the representations are calculated inde- pendently. To address the problem, some other deep methods have been proposed to directly learn the interaction relation- ship between the two texts, including DeepMatch [Lu and Li, 2013], ARC-II [Hu et al., 2014], and MatchPyramid [Pang et al., 2016] etc. All these models conducts the matching through a hierarchical matching structure: the global inter- action between two texts is a composition of different levels of the local interactions, such as word level and phrase level interactions.
In all of these methods, the mechanism on the generation of the complicated interaction relationship between two texts is not clear, and thus lack of interpretability. In this pa- per, we propose to tackle the problem in a recursive man- ner. Specifically, we view the generation of the global in- teractions as a recursive process. Given two texts S1 = {w1,w2,··· ,wm} and S2 = {v1,v2,··· ,vn}, the interac- tion at each position (i,j) (i.e. interaction between S1[1:i] and S2[1:j]) is a composition of the interactions between their prefixes (i.e. three interactions, S1 [1:i−1]∼S2 [1:j ], S1 [1:i]∼S2 [1:j −1], S1 [1:i−1]∼S2 [1:j −1]), and the word level interaction at this position (i.e. the interaction between wi and vj ), where S [1:c] stands for the prefix consisting of the previous c words of text S. Compared with previous hier- archical matching structure, the recursive matching structure can not only capture the interactions between nearby words, but also take the long distant interactions into account.
Based on the above idea, we propose a novel deep architec- ture, namely Match-SRNN, to model the recursive matching structure. Firstly, a similarity tensor is constructed to capture the word level interactions between two texts, where each el- ement ⃗sij stands for a similarity vector between two words
arXiv:1604.04378v1 [cs.CL] 15 Apr 2016
from different texts. Then a spatial (2D) recurrent neural net- work (spatial RNN) with gated recurrent units is applied to the tensor. Specifically, the representation at each position ⃗hij can be viewed as the interactions between the two pre- fixes, i.e. S1[1:i] and S2[1:j]. It is determined by four factors:
⃗hi−1,j,⃗hi,j−1,⃗hi−1,j−1 and the input word level interaction ⃗sij, depending on the corresponding gates, zt,zl,zd, and zi, respectively. Finally, the matching score is produced by a linear scoring function on the representation of the global in- teraction ⃗hmn, obtained by the aforementioned spatial RNN.
We show that Match-SRNN can well approximate the dy- namic programming process of longest common subsequence (LCS) problem [Wikipedia, -]. Furthermore, our simulation experiments show that a clear matching path can be obtained by backtracking the maximum gates at each position, similar to that in LCS. Thus, there is a clear interpretation on how the global interaction is generated in Match-SRNN.
We conducted experiments on question answering and pa- per citation tasks to evaluate the effectiveness of our model. The experimental results showed that Match-SRNN can sig- nificantly outperform existing deep models. Moreover, to vi- sualize the learned matching structure, we showed the match- ing path of two texts sampled from the real data.
approach is consistent with users’ experience that the match- ing degree between two sentences can be determined once the meanings of them being well captured. However, it is usually difficult for these methods to model the complicated inter- action relationship between two texts, especially when they have already been represented as a compact vector [Lu and Li, 2013; Bahdanau et al., 2014].
The other paradigm turns to directly model the interaction relationship of two texts. Specifically, the interaction is rep- resented as a dense vector, and then the matching score can be produced by integrating such interaction. Most existing work of this paradigm create a hierarchical matching struc- ture, i.e. the global interaction between two texts is generated by compositing the local interactions hierarchically. For ex- ample, DeepMatch [Lu and Li, 2013] models the generation of the global interaction between two texts as integrating local interactions based on hierarchies of the topics. MatchPyra- mid [Pang et al., 2016] uses a CNN to model the generation of the global interaction as an abstraction of the word level and phrase level interactions. Defining the matching struc- ture hierarchically has limitations, since hierarchical match- ing structure usually relies on a fixed window size for com- position, the long distant dependency between the local inter- actions cannot be well captured in this kind of models.
3 The Recursive Matching Structure
In all existing methods, the mechanism of semantic matching is complicated and hard to interpret. In mathematics and com- puter science, when facing a complicated object, a common method of simplification is to divide a problem into subprob- lems of the same type, and try to solve the problems recur- sively. This is the well-known thinking of recursion. In this paper, we propose to tackle the semantic matching problem recursively. The recursive rule is defined as follows.
Definition 1 (Recursive Matching Structure) Given two texts S1={w1,··· ,wm} and S2={v1,··· ,vn}, the in- teraction between prefixes S1[1:i]={w1, · · · , wi} and
S2[1:j]={v1, · · · , vj } (denoted as ⃗hij ) is composited by the interactions between the sub-prefixes as well as the word level interaction of the current position, as shown by the following equation:
⃗hij = f(⃗hi−1,j,⃗hi,j−1,⃗hi−1,j−1,⃗s(wi,vj)), (1) where ⃗s(wi , vj ) stands for the interaction between words wi
andvj.
Figure 1 illustrates an example of the recursive match-
2
The contributions of this paper can be summarized as:
• •
•
Theideaofmodelingthemechanismofsemanticmatch- ing recursively, i.e. the recursive matching structure.
Theproposalofanewdeeparchitecture,namelyMatch- SRNN, to model the recursive matching structure. Ex- perimental results showed that Match-SRNN can signif- icantly improve the performances of semantic matching, compared with existing deep models.
The reveal of the relationship between Match-SRNN and the LCS, i.e. Match-SRNN can reproduce the matching path of LCS in an exact matching scenario.
Related Work
Existing deep learning methods for semantic matching can be categorized into two groups.
One paradigm focuses on representing each text to a dense vector, and then compute the matching score based on the similarity between these two vectors. For example, DSSM [Huang et al., 2013] uses a multi-layer fully connected neural network to encode a query (or a document) as a vec- tor. CDSSM [Shen et al., 2014] and ARC-I [Hu et al., 2014] utilize convolutional neural network (CNN), while LSTM- RNN [Palangi et al., 2015] adopts recurrent neural network with long short term memory (LSTM) units to better rep- resent a sentence. Different from above work, CNTN [Qiu and Huang, 2015] uses a neural tensor network to model the interaction between two sentences instead of using the cosine function. With this way, it can capture more com- plex matching relations. Some methods even try to match two sentences with multiple representations, such as words, phrases, and sentences level representations. Examples in- clude RAE [Socher et al., 2011], BiCNN [Yin and Schu ̈tze, 2015a],MultiGranCNN[YinandSchu ̈tze,2015b],andMV- LSTM [Wan et al., 2016]. In general, the idea behind the
structure for sentences S1={The cat sat on the mat} S2={The dog played balls on the floor}. Consider-
ing
and
ing the interaction between S1[1:3]={The cat sat} and
S2[1:4]={The dog played balls} (i.e. ⃗h34), the recursive matching structure defined above indicates that it is the com- position of the interactions between their prefixes (i.e. ⃗h24, ⃗h33, and ⃗h23) and the word level interaction between ‘sat’ and ‘balls’, where ⃗h24 stands for the interaction between
S1[1:2]={Thecat}andS2[1:4]={Thedogplayedballs},⃗h33 denotes the interaction between S1[1:3]={The cat sat} and

      The cat sat on the mat.
The dog played balls on the floor.
Figure 1: Illustration of the recursive matching structure.
S2[1:3]={The dog played}, and ⃗h23 denotes the interaction between S1[1:2]={The cat} and S2[1:3]={The dog played}. We can see that the most important interaction, i.e. the interaction between S1 [1:3]={The cat sat} and S2[1:3]={The dog played}, has been utilized for represent-
ing ⃗h34, which consists well with the human understanding. Therefore, it is expected that this recursive matching struc- ture can well capture the complicated interaction relationship between two texts because all of the interactions between prefixes have been taken into consideration. Compared with the hierarchical one, the recursive matching structure is able to capture long-distant dependency among interactions.
4 Match-SRNN
In this section, we introduce a new deep architecture, namely Match-SRNN, to model the recursive matching structure. As shown in Figure 2, Match-SRNN consists of three compo- nents: (1) a neural tensor network to capture the word level interactions; (2) a spatial RNN applied on the word interac- tion tensor to obtain the global interaction; (3) a linear scoring function to obtain the final matching score.
4.1 Neural Tensor Network
In Match-SRNN, a neural tensor network is first utilized to capture the basic interactions between two texts, i.e. word level interactions. Specifically, each word is first represented as a distributed vector. Given any two words wi and vj , and their vectors u(wi) and u(vj), the interaction between them can be represented as a vector:
Figure 2: The architecture of Match-SRNN.
and S1 [1:i−1]∼S2 [1:j −1], denoted as ⃗hi−1,j , ⃗hi,j −1 , and
⃗hi−1,j −1 , respectively, the interaction between prefixes S1 [1:i] and S2 [1:j ] can be represented as follows:
⃗hij = f(⃗hi−1,j,⃗hi,j−1,⃗hi−1,j−1,⃗sij). (2) Therefore we can see that spatial RNN can naturally model
the recursive matching structure defined in Equation (1).
For function f , we have different choices. The basic RNN usually uses a non-linear full connection layer as f . This type of function is easy for computing while often suffers from the gradient vanishing and exploding problem [Pascanu et al., 2013]. Therefore, many variants of RNN has been proposed, such as Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997], Gated Recurrent Units (GRU) [Cho et al., 2014] and Grid LSTM [Kalchbrenner et al., 2015]. Here, we adopt GRU since it is easy to implement and has close relationship with LCS as discussed in the following sections.
GRU is proposed to utilize several gates to tackle the afore- mentioned problems of basic RNN, and has shown excellent performance for tasks such as machine translation [Cho et al., 2014]. In this paper, we extend traditional GRU for se- quences (1D-GRU) to spatial GRU. Figure 3 describes clearly about the extensions.
For 1D-GRU , given a sentence S=(x1,x2,···,xT), where ⃗xt stands for the embedding of the t-th words, the rep- resentation of position t, i.e. ⃗ht, can be computed as follows:
⃗z =σ(W(z)⃗xt + U(z)⃗ht−1), ⃗r=σ(W(r)⃗xt + U(r)⃗ht−1), ⃗h′t =φ(W⃗xt+U(⃗r ⊙⃗ht−1)), ⃗ht=(⃗1 − ⃗z) ⊙⃗ht−1+⃗z ⊙⃗h′t,
where ⃗ht−1 is the representation of position t−1, W(z),U(z),W(r),U(r),W and U are the parameters, ⃗z
is the updating gate which tries to control whether to propa- gate the old information to the new states or to write the new generated information to the states, and ⃗r is the reset gate which tries to reset the information stored in the cells when generating new candidate hidden states.
When extending to spatial GRU, context information will come from three directions for a given position (i, j ), i.e. (i−1, j ), (i, j −1) and (i−1, j −1), therefore, we will have four updating gates ⃗z, denoted as ⃗zl, ⃗zt, ⃗zd and ⃗zi, and three reset gates ⃗r, denoted as ⃗rl, ⃗rt, ⃗rd. The function f is com-
       S2 S1
Word Interaction Tensor Spatial RNN Linear Layer
                                                                                                                                                               S1[1:2] The cat sat on the mat. S1[1:3]
S2[1:3] The dog played balls on the floor. S2[1:4]
􏰀u(w )􏰁 ⃗sij =F(u(wi)TT[1:c]u(vj)+W i
+⃗b), where T i , i ∈ [1, ..., c] is one slice of the tensor parameters,
W and ⃗b are parameters of the linear part. F is a non-linear function, and we use rectifier F (z) = max(0, z) in this paper. The interaction can also be represented as a similarity score, such as cosine. We adopt neural tensor network here because it can capture more complicated interactions [Socher
et al., 2013a; Socher et al., 2013b; Qiu and Huang, 2015]. 4.2 Spatial RNN
The second step of Match-SRNN is to apply a spatial RNN to the word level interaction tensor. Spatial RNN, also re- ferred to as two dimensional RNN (2D-RNN), is a special case of multi-dimensional RNN [Graves et al., 2007; Graves and Schmidhuber, 2009; Theis and Bethge, 2015]. Accord- ing to spatial RNN, given the representations of interac- tions between prefixes S1 [1:i−1]∼S2 [1:j ], S1 [1:i]∼S2 [1:j −1]
u(vj )

 Figure 3: Illustration of Gated Recurrent Units. The left one is 1D-GRU, where different hs are denoted as one node. The right one is the spatial GRU used in this paper.
puted as follows.
⃗qT =[⃗hTi−1,j,⃗hTi,j−1,⃗hTi−1,j−1,⃗sTij]T,
⃗rl = σ(W(rl)⃗q +⃗b(rl)), ⃗rt = σ(W(rt)⃗q +⃗b(rt)),
⃗rd =σ(W(rd)⃗q+⃗b(rd)),⃗rT =[⃗rlT,⃗rtT,⃗rdT]T,
⃗zi′ = W(zi)⃗q +⃗b(zi), ⃗zl′ = W(zl)⃗q +⃗b(zl),
⃗zt′ = W(zt)⃗q +⃗b(zt), ⃗zd′ = W(zd)⃗q +⃗b(zd),
[⃗zi , ⃗zl , ⃗zt , ⃗zd ] = SoftmaxByRow([⃗zi′ , ⃗zl′ , ⃗zt′ , ⃗zd′ ]), (3)
⃗h′ij=φ(W⃗sij + U(⃗r ⊙ [⃗hTi,j−1,⃗hTi−1,j,⃗hTi−1,j−1]T ) +⃗b),
⃗hij=⃗zl ⊙⃗hi,j−1+⃗zt ⊙⃗hi−1,j+⃗zd ⊙⃗hi−1,j−1+⃗zi ⊙⃗h′ij, (4)
where U , W ’s, and b’s are parameters, and SoftmaxByRow is a function to conduct softmax on each dimension across the four gates, that is:
where M(S1,S2+) and M(S1,S2−) are the corresponding matching scores.
All parameters of the model, including the parameters of word embedding, neural tensor network, spatial RNN are jointly trained by BackPropagation and Stochastic Gradient Descent. Specifically, we use AdaGrad [Duchi et al., 2011] on all parameters in the training process.
5 Discussion
In this section, we show the relationship between Match- SRNN and the well known longest common subsequence (LCS) problem.
5.1 Theoretical Analysis
The goal of LCS problem is to find the longest subsequence common to all sequences in a set of sequences (often just two sequences). In many applications such as DNA detection, the lengths of LCS are used to define the matching degree be- tween two sequences.
Formally, given two sequences, e.g. S1={x1, · · · , xm} and S2={y1,···,yn}, let c[i,j] represents the length of LCS between S1 [1:i] and S2 [1:j ]. The length of LCS be- tween S1 and S2 can be obtained by the following recur- sive progress, with each step c[i,j] determined by four fac- tors, i.e. c[i−1, j −1], c[i−1, j ], c[i, j −1], and the matching between xi and yj .
c[i,j]=max(c[i,j−1],c[i−1,j],c[i−1,j−1]+I{xi=yj}), (7)
where I{xi =yj } is an indicator function, it is equal to 1 if xi = yj , and 0 otherwise. c[i, j ]=0 if i=0 or j =0.
Match-SRNN has strong connection to LCS. To show this, we first degenerate the Match-SRNN to model an ex- act matching problem, by replacing the neural tensor network with a simple indicator function which returns 1 if the two words are identical and 0 otherwise, i.e. sij=I{xi=yj}. The dimension of spatial GRU cells is also set to 1. The reset gates of spatial GRU are disabled since the length of LCS is accu- mulated depending on all the past histories. Thus, Equation (4) can be degenerated as
hij =zl ·hi,j−1 +zt ·hi−1,j +zd ·hi−1,j−1 +zi ·h′ij,
where zl · hi,j−1, zt · hi−1,j , and zd · hi−1,j−1 + zi · h′ij cor- respond to the terms c[i, j−1], c[i−1, j], and c[i−1, j−1] +
I{xi =yj } in Equation (7), respectively. Please note that zl , zt , zd and zi are calculated by SoftmaxByRow, and thus can approximate the max operation in Equation (7). By ap- propriately setting zi and zd and other parameters of Match- SRNN, zd · h
